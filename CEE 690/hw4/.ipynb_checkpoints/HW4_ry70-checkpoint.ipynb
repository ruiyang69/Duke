{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=pd.read_csv('hw3_problem_5_y_blind.csv')\n",
    "y_blind=np.array(y.values[:,1],dtype=np.float64)\n",
    "y=pd.read_csv('hw3_problem_5_y_blind_predict_valid.csv')\n",
    "y_blind_pred_validation=np.array(y.values[:,1],dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "fpr, tpr, _ = metrics.roc_curve(y_blind, y_blind_pred_validation)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)\n",
    "My result is worse than expected 0.5 AUC which means the classifier is taking random guesses. I think the problem was that I did not do a train-test split before processing the data. Also, my parameter ranges are too small to get a better result if anything with roc > 0.5 lies outside the range.\n",
    "\n",
    "(c)\n",
    "During HW3 GridSearch, I was using C ranging [0.0001, 1000] and gamma ranging [0.00001, 5000]. C represents the penalty and gamma represents the data distribution. Large C and gamma means less tolerance on errors and could lead to overfitting of training set. Smaller C and gamma are the opposite. By having larger range and possibly more sampling points inside the range, we could try more combinations of C and gamma to give a better result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) \n",
    "CNN takes advantages of repeated, hierarchical structure by sharing weights while multi-layer perception learns each weight independently. \n",
    "\n",
    "(b)\n",
    "1.\n",
    "CNN is preferred that objects might be positioned anywhere in the images. CNN reduces number of parameters needed by sharing weights. So that dogs on the bottom left corner will be classified as the same objects as dogs on the top right corner.\n",
    "\n",
    "2.\n",
    "CNN would be preferred from my opinion because it's good at processing images. But medical images are less random than natural ones so a multi-layer perception could be good too.\n",
    "\n",
    "3.\n",
    "I would prefer using logistic regression. The dataset is well defined and only binary outcomes are needed which gives logistic regression a good performance. \n",
    "\n",
    "4.\n",
    "Logic regression for the same reason as previous one.\n",
    "\n",
    "5.\n",
    "I would prefer kenerl SVM. For two-moon dataset, neighbors would weigh more on predictions of current data point. So SVM would help here. Also, due to its curved nature, a kernel SVM could separate the data non-linearly using kernel tricks.\n",
    "\n",
    "(c)\n",
    "1.\n",
    "Yes, by having high interprebility, experimenter would be able to demonstrate results to other people in a clear way. And the experiment could be conducted again easily.\n",
    "\n",
    "2.\n",
    "In the sense of computation power, a random forest would require way less computation than a well-trained CNN. But due to the nature of randomness, random forests could not give the same or consistent result everytime we run it.\n",
    "\n",
    "3.\n",
    "No. Our predictions are based on previously seen data and try to find the corelation among different components which is not a causal mechanism. \n",
    "\n",
    "4.\n",
    "No. Deep learning is very resouce-consuming in terms of time and computation power. For simple dataset, deep learning would cost too much. So, we need to evaluate the dataset before train the model. And models we use should be evaluated too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c51c8acbb44d028408975220551a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=600), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruiyang/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e00e98fb58824a9db376d841fa601c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.9017000198364258\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class MNIST_Logistic_Regression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cn1 = nn.Linear(784, 500)\n",
    "        self.cn2 = nn.Linear(500, 100)\n",
    "        self.cn3 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.cn1(x))\n",
    "        x = F.relu(self.cn2(x))\n",
    "        x = self.cn3(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "# Load the data\n",
    "mnist_train = datasets.MNIST(root=\"./datasets\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "mnist_test = datasets.MNIST(root=\"./datasets\", train=False, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=100, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=100, shuffle=False)\n",
    "\n",
    "## Training\n",
    "our_model = MNIST_Logistic_Regression()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(our_model.parameters(), lr=0.1)\n",
    "\n",
    "# Iterate through train set minibatchs \n",
    "for images, labels in tqdm(train_loader):\n",
    "    # Zero out the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    x = images.view(-1, 28*28)\n",
    "    y = our_model(x)\n",
    "    cross_entropy = F.cross_entropy(y, labels)\n",
    "    # Backward pass\n",
    "    cross_entropy.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "## Testing\n",
    "correct = 0\n",
    "total = len(mnist_test)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Iterate through test set minibatchs \n",
    "    for images, labels in tqdm(test_loader):\n",
    "        # Forward pass\n",
    "        x = images.view(-1, 28*28)\n",
    "        y = torch.matmul(x, W) + b\n",
    "        \n",
    "        predictions = torch.argmax(y, dim=1)\n",
    "        correct += torch.sum((predictions == labels).float())\n",
    "    \n",
    "print('Test accuracy: {}'.format(correct/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4\n",
    "\n",
    "The first step is to load the data and divide into training and validation groups as usual. Then, a generation function which decides the training rate and the so-far best model trained. Next, we do a fine-tuning, which loads a pretrained model and reset final fully connected layer. After that, we freeze all network except the final layer and does not require gradients. Finally do a trian-evaluate process again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5\n",
    "1.\n",
    "I took about 20 hours on this assignment.\n",
    "\n",
    "2.\n",
    "I adhered to the Duke Honor Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
